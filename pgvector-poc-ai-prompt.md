# AI Assistant Prompt for PostgreSQL pgvector RAG POC

## Instructions for Use

1. Create a new empty repository/directory: `mkdir pgvector-rag-poc && cd pgvector-rag-poc`
2. Initialize git: `git init`
3. Open this directory in your AI coding assistant (Cursor, Windsurf, etc.)
4. Paste the entire `pgvector-poc-specification.md` document as context
5. Paste the prompt below to the AI assistant
6. Let it build the complete POC

---

## Prompt for AI Assistant

You are an expert Python developer and PostgreSQL database engineer. I need you to build a complete proof-of-concept (POC) application based on the detailed specification I've provided.

### Your Task

Build a fully functional PostgreSQL + pgvector RAG (Retrieval-Augmented Generation) proof-of-concept system that demonstrates superior similarity search accuracy compared to ChromaDB.

### Context

I've provided a comprehensive specification document (`pgvector-poc-specification.md`) that contains:
- Complete technical requirements
- Database schema with SQL
- Expected file structure
- Implementation details
- Success criteria
- All necessary configuration

### What You Need to Build

Create a complete, working POC with these components:

#### 1. Docker Infrastructure
- `docker-compose.yml` - PostgreSQL 17 with pgvector, port 5433, health checks
- `init.sql` - Complete database schema with tables, indexes, and triggers
- All configuration as specified in the spec document

#### 2. Python Application
Create the following structure:
```
src/
├── __init__.py
├── database.py          # DB connection, connection pooling
├── embeddings.py        # OpenAI embeddings with CRITICAL normalization
├── ingestion.py         # Document ingestion with metadata
├── search.py            # Similarity search with proper distance conversion
├── collections.py       # Collection CRUD operations
└── cli.py              # CLI using Click with all specified commands
```

#### 3. Configuration & Setup
- `pyproject.toml` - Modern Python project config with all dependencies
- `.python-version` - Specify Python 3.12 for uv
- `uv.lock` - Lock file for reproducible builds (generated by uv)
- `.env.example` - Template for environment variables
- `.gitignore` - Standard Python + Docker ignore patterns
- `README.md` - Complete setup and usage instructions with uv commands

#### 4. Testing
- `tests/sample_documents.py` - Test data with varying similarity levels
- `tests/test_embeddings.py` - Verify normalization works
- `tests/test_search.py` - Validate similarity scores

#### 5. Documentation
- Comprehensive README with:
  - Quick start guide
  - Prerequisites
  - Setup instructions
  - Usage examples for all CLI commands
  - Troubleshooting section
  - Expected results vs ChromaDB

### Critical Implementation Requirements

**⚠️ MUST IMPLEMENT CORRECTLY:**

1. **Vector Normalization** (This is the #1 reason ChromaDB is failing)
   ```python
   def normalize_embedding(embedding):
       """Normalize to unit length - CRITICAL for accurate scores"""
       arr = np.array(embedding)
       norm = np.linalg.norm(arr)
       return (arr / norm).tolist() if norm > 0 else arr.tolist()
   ```
   - Apply to ALL embeddings before storage
   - Apply to query embeddings before search

2. **Distance to Similarity Conversion**
   - pgvector `<=>` returns distance (0-2), not similarity
   - Convert: `similarity = 1 - distance`
   - This gives scores in 0-1 range

3. **HNSW Index Configuration**
   - Use `USING hnsw (embedding vector_cosine_ops)`
   - Parameters: `m=16, ef_construction=64`
   - This is critical for 95%+ recall accuracy

4. **Database Schema**
   - Exactly as specified in the spec document
   - Include all indexes, triggers, and relationships
   - Don't skip anything

5. **CLI Commands**
   Implement all commands from spec:
   - `poc init` - Initialize database
   - `poc collection create/list/delete`
   - `poc ingest file/text/directory`
   - `poc search` with all options
   - `poc test-similarity` - Critical for validation
   - `poc status` - Show connection and stats

### Code Quality Requirements

- ✅ Type hints on all functions
- ✅ Docstrings explaining what each function does
- ✅ Error handling with helpful messages
- ✅ Logging for debugging (use Python `logging` module)
- ✅ Comments explaining WHY, not just WHAT
- ✅ Clean, readable code following PEP 8

### Testing Requirements

Create test documents that validate similarity scores:

**High Similarity Test (expect 0.85-0.95):**
```python
doc = "PostgreSQL is a powerful relational database system"
query = "What is PostgreSQL and what type of database is it?"
```

**Medium Similarity Test (expect 0.60-0.75):**
```python
doc = "Python is a popular programming language for data science"
query = "Tell me about machine learning tools"
```

**Low Similarity Test (expect 0.20-0.40):**
```python
doc = "The weather today is sunny and warm"
query = "How do I configure a database?"
```

### Expected Output

When I run the POC, I should see:

1. **Better Similarity Scores**: 0.7-0.95 for good matches (vs 0.3 with ChromaDB)
2. **Fast Queries**: < 100ms for reasonable dataset sizes
3. **Relevant Results**: Properly ranked by semantic similarity
4. **Easy to Use**: Clear CLI commands with helpful output

### Deliverables Checklist

- [ ] Docker Compose with PostgreSQL 17 + pgvector
- [ ] Complete database schema with all tables and indexes
- [ ] Python application with all specified modules
- [ ] pyproject.toml configured for uv with all dependencies
- [ ] .python-version file specifying Python 3.12
- [ ] Working CLI with all commands from spec (using `uv run poc`)
- [ ] Test suite with sample documents
- [ ] README with complete documentation using uv commands
- [ ] .env.example with all required variables
- [ ] Comments explaining critical concepts (normalization, distance conversion)

### Special Instructions

1. **Set Up Project with uv**
   - Create `.python-version` file with "3.12"
   - Create `pyproject.toml` with all dependencies as specified
   - Use uv for all package management (NOT pip)
   - Include CLI entry point in pyproject.toml: `poc = "src.cli:main"`

2. **Start with Docker and Database First**
   - Get PostgreSQL running
   - Verify pgvector extension loads
   - Create schema and test connection

3. **Then Build Core Functions**
   - Database connection
   - Embedding generation WITH NORMALIZATION
   - Basic insert and search

4. **Add CLI Layer**
   - Implement commands incrementally
   - Test each command with `uv run poc <command>`

5. **Create Tests and Validation**
   - Test similarity scores against expected ranges
   - Run with `uv run pytest`
   - Document results in README

6. **Polish and Document**
   - Clean up code
   - Add comprehensive README with uv commands
   - Include troubleshooting section

### Important Notes

- **Use uv for ALL package management**: NOT pip, NOT poetry - only uv
- **Use pyproject.toml**: Modern Python project configuration
- **Use PostgreSQL 17**: Specified in docker-compose as `pgvector/pgvector:pg17`
- **Use Python 3.12**: Specified in .python-version file
- **Use Latest Stable Versions**: LangChain 0.3.x, psycopg3, OpenAI 1.50+
- **Embedding Model**: text-embedding-3-small (1536 dims, $0.02/1M tokens)
  - 6.5x cheaper than large with near-equivalent performance
  - Vector dimension: 1536 (not 3072)
- **Port 5433**: To avoid conflicts with existing PostgreSQL
- **OpenAI API Key Required**: User will provide in .env file
- **Normalization is CRITICAL**: This is the key to fixing the 0.3 score problem
- **HNSW Index**: Essential for accuracy, don't skip this
- **CLI Entry Point**: Configure in pyproject.toml so `uv run poc` works

### Success Criteria

The POC is successful when:

✅ Similarity scores are 0.7-0.95 for semantically similar content  
✅ Scores are significantly better than the reported 0.3 from ChromaDB  
✅ Search returns relevant, properly ranked results  
✅ All CLI commands work correctly  
✅ Code is clean, documented, and understandable  
✅ README makes it easy for someone else to run the POC  

### What NOT to Do

❌ Don't skip vector normalization - it's the most critical piece  
❌ Don't use IVFFlat index - use HNSW for better accuracy  
❌ Don't forget to convert distance to similarity  
❌ Don't make it overly complex - keep it simple and focused  
❌ Don't skip the test data - validation is critical  

### When You're Done

Provide:
1. Summary of what was built
2. Instructions for running the POC
3. What the expected results should look like
4. Any gotchas or important notes
5. Recommendations for integrating into the main RAG Retriever

---

## Additional Context

The user is migrating from ChromaDB to pgvector because:
- ChromaDB showing unexpectedly low similarity scores (0.3)
- Need better accuracy for RAG applications
- Want to leverage existing PostgreSQL infrastructure
- Need both local and cloud deployment support

This POC will validate that pgvector solves these problems before integrating into the larger RAG Retriever codebase.

**Start building now. Create all the files, implement all the features, and make it work correctly. Focus on getting the similarity scores right - that's the primary goal.**

