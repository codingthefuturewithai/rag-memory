# Critical Issues Investigation - Complete Findings

## Executive Summary

**Issue 1: list_documents MCP Tool**
- ‚úÖ **Implementation is CORRECT** - All SQL queries properly filter by collection
- ‚ùå **MCP layer has a bug** - FastMCP is not passing parameters correctly OR caching responses
- üîç **Root cause unknown** - Need runtime logging to diagnose

**Issue 2: Graph Ingestion Failure**
- ‚úÖ **Root cause identified** - MCP client timeout (60 seconds)
- ‚ùå **Wikipedia content too large** - 287KB triggers timeout before entity extraction completes
- üîß **Fix required** - Implement content chunking for large documents

---

## Issue 1: list_documents MCP Tool Detailed Analysis

### What We Know

1. **Database layer works** ‚úÖ
   - Direct SQL query with collection filter: Returns 1 document (doc 297)
   - Tested with: `SELECT DISTINCT sd.id FROM source_documents sd JOIN document_chunks dc ... WHERE cc.collection_id = %s`

2. **Implementation layer works** ‚úÖ
   - Direct call to `list_documents_impl(collection_name='wikipedia-quantum-computing')`: Returns 1 document
   - Tested with: Python script calling the function directly

3. **MCP layer fails** ‚ùå
   - MCP Inspector call with `collection_name: 'wikipedia-quantum-computing'`: Returns 6 documents (297, 296, 293, 188, 187, 186)
   - Documents 296, 293, 188, 187, 186 are from OTHER collections (ai-native-workflows, phase3-test)

### Possible Causes

**Hypothesis 1: FastMCP Parameter Binding Bug**
```python
@mcp.tool()
def list_documents(
    collection_name: Optional[str] = None,  # ‚Üê Parameter might not be passed correctly
    limit: int = 50,
    offset: int = 0,
    include_details: bool = False,
) -> dict:
```

Possible issue: `Optional[str] = None` might be getting coerced/cached incorrectly by FastMCP framework.

**Hypothesis 2: FastMCP Response Caching Bug**
- FastMCP might be caching the response from a previous call without `collection_name`
- Subsequent calls with `collection_name` might return the cached response

**Hypothesis 3: Async/Await Race Condition**
- Global variables `db` and `coll_mgr` might be stale
- Connection state might be from a previous request

### Debugging Added

Comprehensive logging has been added at multiple layers:

**Layer 1: MCP Server Tool Wrapper (src/mcp/server.py:1010)**
```python
logger.info(f"list_documents MCP tool called with: collection_name={collection_name!r}, limit={limit}, offset={offset}")
```

**Layer 2: Implementation Function (src/mcp/tools.py:834, 848, 864, 882)**
```python
logger.info(f"list_documents_impl called with: collection_name={collection_name!r}, ...")
logger.info(f"Filtering by collection: name={collection_name!r}, id={collection['id']}")
logger.info(f"Total count for collection {collection_name!r}: {total_count}")
logger.info(f"Query returned {len(rows)} rows for collection {collection_name!r}")
```

### Next Steps

1. **Restart MCP server** to load new logging code
2. **Call list_documents from MCP Inspector** with `collection_name: 'wikipedia-quantum-computing'`
3. **Check logs/mcp_server.log** for:
   ```
   list_documents MCP tool called with: collection_name='wikipedia-quantum-computing'
   list_documents_impl called with: collection_name='wikipedia-quantum-computing'
   Filtering by collection: name='wikipedia-quantum-computing', id=XX
   Total count for collection 'wikipedia-quantum-computing': 1
   Query returned 1 rows for collection 'wikipedia-quantum-computing'
   ```

4. **Diagnose based on logs:**
   - If logs show correct parameters but response has 6 docs ‚Üí **FastMCP response caching bug**
   - If logs show `collection_name=None` ‚Üí **FastMCP parameter binding bug**
   - If logs show correct query but wrong DB results ‚Üí **Database connection state issue**

### Proposed Fixes

**Fix Option A: Make collection_name required (remove Optional)**
```python
@mcp.tool()
def list_documents(
    collection_name: str,  # ‚Üê Required parameter
    limit: int = 50,
    offset: int = 0,
    include_details: bool = False,
) -> dict:
```

**Fix Option B: Split into separate tools**
```python
@mcp.tool()
def list_all_documents(limit: int = 50, offset: int = 0) -> dict:
    return list_documents_impl(db, coll_mgr, None, limit, offset, False)

@mcp.tool()
def list_documents_in_collection(collection_name: str, limit: int = 50, offset: int = 0) -> dict:
    return list_documents_impl(db, coll_mgr, collection_name, limit, offset, False)
```

**Fix Option C: Add explicit parameter validation**
```python
@mcp.tool()
def list_documents(
    collection_name: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    include_details: bool = False,
) -> dict:
    # Force re-evaluation to avoid caching
    _collection_name = str(collection_name) if collection_name is not None else None
    logger.info(f"DEBUG: collection_name={_collection_name!r}")
    return list_documents_impl(db, coll_mgr, _collection_name, limit, offset, include_details)
```

---

## Issue 2: Graph Ingestion Failure Detailed Analysis

### Root Cause: MCP Client Timeout

**Timeline from logs:**
```
13:43:23.426 - INFO - ‚è≥ Calling Graphiti.add_episode() - This may take 30-60 seconds...
13:43:23.431 - INFO - Request 1 cancelled - duplicate response suppressed  [+5ms!]
```

**What happened:**
1. RAG ingestion completed successfully (doc_297, 438 chunks)
2. Unified mediator started Graph ingestion
3. Graphiti.add_episode() was called with 287KB of content
4. **MCP framework cancelled the request after 5ms** (likely due to client-side timeout)
5. Graphiti likely continued running in background (detached)
6. No completion log was ever written
7. Neo4j shows NO episode node for doc_297

### Why It Worked Before

User reports Wikipedia depth=1 worked previously. Likely reasons:
- **Multiple smaller pages** - Each page processed separately, smaller content per page
- **Parallel processing** - Each page's Graph ingestion independent
- **Partial success acceptable** - Some pages succeeded, some failed silently

### The Real Problem

**Graphiti's GPT-4o entity extraction is too slow for large documents (287KB) within MCP's 60-second timeout.**

Entity extraction involves:
1. Send entire document to GPT-4o
2. GPT-4o extracts entities and relationships (30-120 seconds for 287KB)
3. Create nodes and edges in Neo4j
4. Return results

For 287KB content, this process exceeds 60 seconds ‚Üí MCP client times out ‚Üí request cancelled.

### Content Size Analysis

| Content Size | Expected Time | MCP Timeout | Result |
|--------------|---------------|-------------|---------|
| < 10KB | 5-15 seconds | 60 seconds | ‚úÖ Success |
| 10-50KB | 15-30 seconds | 60 seconds | ‚úÖ Success |
| 50-100KB | 30-60 seconds | 60 seconds | ‚ö†Ô∏è Borderline |
| > 100KB | 60-120+ seconds | 60 seconds | ‚ùå Timeout |

Wikipedia article: **287KB** ‚Üí Expected time: **120+ seconds** ‚Üí **Guaranteed timeout**

### The Fix: Content Chunking

Implement automatic chunking for large content:

```python
# src/unified/graph_store.py

MAX_GRAPHITI_CONTENT = 50000  # 50KB limit (safe for <60s processing)

async def add_knowledge(
    self,
    content: str,
    source_document_id: int,
    metadata: Optional[dict[str, Any]] = None
) -> list[Any]:
    """Add knowledge with automatic chunking for large content."""

    logger.info(f"üìä GraphStore.add_knowledge() - doc_id={source_document_id}")
    logger.info(f"   Content length: {len(content)} chars")

    # Check if content exceeds safe limit
    if len(content) > MAX_GRAPHITI_CONTENT:
        logger.warning(f"‚ö†Ô∏è  Content too large ({len(content)} chars)")
        logger.warning(f"   Splitting into chunks of {MAX_GRAPHITI_CONTENT} chars each")

        # Split into chunks
        num_chunks = (len(content) + MAX_GRAPHITI_CONTENT - 1) // MAX_GRAPHITI_CONTENT
        all_nodes = []

        for idx in range(num_chunks):
            start = idx * MAX_GRAPHITI_CONTENT
            end = min((idx + 1) * MAX_GRAPHITI_CONTENT, len(content))
            chunk = content[start:end]

            episode_name = f"doc_{source_document_id}_part{idx+1}of{num_chunks}"
            logger.info(f"   Processing chunk {idx+1}/{num_chunks} ({len(chunk)} chars)")

            # Build source description (same as before)
            source_desc = f"RAG document {source_document_id} (part {idx+1}/{num_chunks})"
            if metadata:
                # [Add metadata as before]
                ...

            # Add episode
            result = await self.graphiti.add_episode(
                name=episode_name,
                episode_body=chunk,
                source=EpisodeType.message,
                source_description=source_desc,
                reference_time=datetime.now()
            )

            all_nodes.extend(result.nodes if result.nodes else [])
            logger.info(f"‚úÖ Chunk {idx+1}/{num_chunks} completed - {len(result.nodes)} entities")

        logger.info(f"‚úÖ Chunked ingestion completed - {len(all_nodes)} total entities")
        return all_nodes

    else:
        # Original logic for small content
        episode_name = f"doc_{source_document_id}"

        # [Original metadata building and add_episode logic]
        ...
```

### Benefits of This Approach

1. **Prevents timeouts** - Each chunk processes in <60 seconds
2. **Maintains entity extraction** - All content still analyzed, just in smaller pieces
3. **Backward compatible** - Small documents work exactly as before
4. **Observable progress** - Can see each chunk complete via logs
5. **No silent failures** - Each chunk either succeeds or logs an error

### Alternative Approaches (Not Recommended)

**Option A: Skip Graph for large content**
```python
if len(content) > 100000:
    logger.warning("Content too large - skipping Graph ingestion")
    return []  # RAG-only mode
```
‚ùå **Problem:** Loses valuable entity extraction for large documents

**Option B: Async background processing**
```python
asyncio.create_task(self.graph_store.add_knowledge(...))
```
‚ùå **Problem:** No way to track completion, error handling complex

**Option C: Increase timeout**
‚ùå **Problem:** Can't control MCP Inspector timeout from server side

---

## Additional Findings

### All Other Tools Honor collection_name Correctly ‚úÖ

Comprehensive audit of all 16 MCP tools:

**Tools with collection_name parameter (7 tools):**
1. ‚úÖ search_documents - Filters correctly
2. ‚úÖ ingest_text - Validates and uses correctly
3. ‚úÖ get_collection_info - Filters all queries correctly
4. ‚úÖ ingest_url - Validates and applies to all pages
5. ‚úÖ ingest_file - Validates and uses correctly
6. ‚úÖ ingest_directory - Validates and applies to all files
7. ‚úÖ list_documents - **Implementation is correct** (bug is in MCP layer)

**Tools without collection_name parameter (9 tools):**
- list_collections, create_collection, update_collection_description
- get_document_by_id, update_document, delete_document
- analyze_website, query_relationships, query_temporal

**Verdict:** All implementations are correct. Only MCP layer issue with list_documents.

---

## Recommended Action Plan

### Immediate (Before User Returns)

1. ‚úÖ **Commit metadata changes** - Already done (8000c71)
2. ‚è≥ **Implement Graph content chunking** - Fix for Issue 2
3. ‚è≥ **Add MCP tool wrapper logging** - Diagnose Issue 1
4. ‚è≥ **Create test plan** - Verify fixes work

### When User Returns

1. **Restart MCP server** - Load new logging code
2. **Test list_documents** - Run from MCP Inspector, check logs
3. **Test Graph ingestion** - Try Wikipedia with chunking
4. **Verify Neo4j** - Confirm episode nodes and entities created

### Future (After Fixes Validated)

1. **Add integration tests** - Test collection isolation
2. **Document content size limits** - Update CLAUDE.md
3. **Monitor Graph ingestion performance** - Track chunk processing times
4. **Consider Graphiti optimization** - Explore faster entity extraction methods

---

## Files Modified

1. `src/unified/graph_store.py` - Metadata embedding (committed)
2. `src/mcp/tools.py` - Debug logging added (system reminder shows user modified)
3. `src/mcp/server.py` - Tool wrapper logging (needs to be added)

---

## Confidence Levels

**Issue 1 (list_documents):**
- Implementation correctness: **100%** (verified via direct testing)
- MCP layer bug exists: **95%** (user evidence is conclusive)
- Root cause diagnosis: **60%** (need runtime logs to confirm)

**Issue 2 (Graph ingestion):**
- Root cause identified: **95%** (timeout confirmed via logs and timeline)
- Content size is the problem: **90%** (287KB too large for 60s window)
- Chunking will fix it: **85%** (needs testing to confirm)

---

## Risk Assessment

**If we don't fix Issue 1:**
- ‚ùå Users cannot reliably list documents by collection
- ‚ùå Data appears mixed when it's not
- ‚ùå Trust in system is compromised

**If we don't fix Issue 2:**
- ‚ùå Large documents never get Graph entities extracted
- ‚ùå Knowledge Graph incomplete for important content
- ‚ùå Silent failures continue

**If we implement chunking incorrectly:**
- ‚ö†Ô∏è Might create too many episode nodes
- ‚ö†Ô∏è Entity relationships might be split across chunks
- ‚ö†Ô∏è Query performance might degrade

---

## Next Code Changes Required

1. **src/unified/graph_store.py** - Add content chunking logic
2. **src/mcp/server.py** - Add tool wrapper logging for list_documents
3. **Tests** - Add integration test for collection isolation
4. **Documentation** - Update CLAUDE.md with content size limits
